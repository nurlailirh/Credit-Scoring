{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "import pandas as pd\n",
    "data = pd.read_excel('raw-dataset.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29366, 18)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['durasi',\n",
       " 'interest',\n",
       " 'net_income',\n",
       " 'additional_income',\n",
       " 'plafon',\n",
       " 'new_debitur',\n",
       " 'biz_ownership',\n",
       " 'loyal_customer',\n",
       " 'liquidation_value',\n",
       " 'age',\n",
       " 'marriage_status',\n",
       " 'gender',\n",
       " 'edu_code',\n",
       " 'edu',\n",
       " 'job_code',\n",
       " 'job_kind',\n",
       " 'monthly_installment',\n",
       " 'Good_Not_Good']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data columns and rows\n",
    "print(data.shape)\n",
    "list(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>durasi</th>\n",
       "      <th>interest</th>\n",
       "      <th>net_income</th>\n",
       "      <th>additional_income</th>\n",
       "      <th>plafon</th>\n",
       "      <th>new_debitur</th>\n",
       "      <th>biz_ownership</th>\n",
       "      <th>loyal_customer</th>\n",
       "      <th>liquidation_value</th>\n",
       "      <th>age</th>\n",
       "      <th>marriage_status</th>\n",
       "      <th>gender</th>\n",
       "      <th>edu_code</th>\n",
       "      <th>edu</th>\n",
       "      <th>job_code</th>\n",
       "      <th>job_kind</th>\n",
       "      <th>monthly_installment</th>\n",
       "      <th>Good_Not_Good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.871000e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15000000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54.0</td>\n",
       "      <td>B</td>\n",
       "      <td>F</td>\n",
       "      <td>S1</td>\n",
       "      <td>Sarjana</td>\n",
       "      <td>ADMI</td>\n",
       "      <td>Administrasi Umum / Supervisor</td>\n",
       "      <td>778700.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.050000e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50000000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63.0</td>\n",
       "      <td>B</td>\n",
       "      <td>F</td>\n",
       "      <td>S1</td>\n",
       "      <td>Sarjana</td>\n",
       "      <td>ADMI</td>\n",
       "      <td>Administrasi Umum / Supervisor</td>\n",
       "      <td>2683300.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.100000e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5000000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>B</td>\n",
       "      <td>F</td>\n",
       "      <td>S1</td>\n",
       "      <td>Sarjana</td>\n",
       "      <td>ADMI</td>\n",
       "      <td>Administrasi Umum / Supervisor</td>\n",
       "      <td>270800.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.590000e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15000000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>B</td>\n",
       "      <td>F</td>\n",
       "      <td>S1</td>\n",
       "      <td>Sarjana</td>\n",
       "      <td>ADMI</td>\n",
       "      <td>Administrasi Umum / Supervisor</td>\n",
       "      <td>1013300.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.130000e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35000000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.0</td>\n",
       "      <td>B</td>\n",
       "      <td>F</td>\n",
       "      <td>SD</td>\n",
       "      <td>SD</td>\n",
       "      <td>ADMI</td>\n",
       "      <td>Administrasi Umum / Supervisor</td>\n",
       "      <td>3336700.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   durasi  interest    net_income  additional_income      plafon  new_debitur  \\\n",
       "0      24       8.0  2.871000e+10                0.0  15000000.0          NaN   \n",
       "1      24       8.0  5.050000e+10                0.0  50000000.0          NaN   \n",
       "2      24       8.0  1.100000e+10                0.0   5000000.0          NaN   \n",
       "3      18       8.0  1.590000e+10                0.0  15000000.0          NaN   \n",
       "4      12       8.0  5.130000e+11                0.0  35000000.0          NaN   \n",
       "\n",
       "   biz_ownership  loyal_customer  liquidation_value   age marriage_status  \\\n",
       "0              0               0                NaN  54.0               B   \n",
       "1              0               0                NaN  63.0               B   \n",
       "2              1               0                NaN  50.0               B   \n",
       "3              1               0                NaN  50.0               B   \n",
       "4              0               0                NaN  52.0               B   \n",
       "\n",
       "  gender edu_code      edu job_code                        job_kind  \\\n",
       "0      F       S1  Sarjana     ADMI  Administrasi Umum / Supervisor   \n",
       "1      F       S1  Sarjana     ADMI  Administrasi Umum / Supervisor   \n",
       "2      F       S1  Sarjana     ADMI  Administrasi Umum / Supervisor   \n",
       "3      F       S1  Sarjana     ADMI  Administrasi Umum / Supervisor   \n",
       "4      F       SD       SD     ADMI  Administrasi Umum / Supervisor   \n",
       "\n",
       "   monthly_installment  Good_Not_Good  \n",
       "0             778700.0              1  \n",
       "1            2683300.0              1  \n",
       "2             270800.0              1  \n",
       "3            1013300.0              1  \n",
       "4            3336700.0              1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print first columns\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29366 entries, 0 to 29365\n",
      "Data columns (total 18 columns):\n",
      "durasi                 29366 non-null int64\n",
      "interest               29366 non-null float64\n",
      "net_income             29366 non-null float64\n",
      "additional_income      29366 non-null float64\n",
      "plafon                 29366 non-null float64\n",
      "new_debitur            116 non-null float64\n",
      "biz_ownership          29366 non-null int64\n",
      "loyal_customer         29366 non-null int64\n",
      "liquidation_value      107 non-null float64\n",
      "age                    29255 non-null float64\n",
      "marriage_status        29097 non-null object\n",
      "gender                 29161 non-null object\n",
      "edu_code               28891 non-null object\n",
      "edu                    28891 non-null object\n",
      "job_code               29101 non-null object\n",
      "job_kind               27150 non-null object\n",
      "monthly_installment    28524 non-null float64\n",
      "Good_Not_Good          29366 non-null int64\n",
      "dtypes: float64(8), int64(4), object(6)\n",
      "memory usage: 4.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove unnecessary columns\n",
    "data.drop(['job_kind'], axis = 1, inplace = True) #already represented in job code\n",
    "data.drop(['edu'], axis = 1, inplace = True) #already represented in edu code\n",
    "data.drop(['liquidation_value'], axis = 1, inplace = True) #insufficient data\n",
    "data.drop(['new_debitur'], axis = 1, inplace = True) #insufficient data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\site-packages\\sklearn\\utils\\deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Handling missing values\n",
    "\n",
    "#filling with mean\n",
    "from sklearn.preprocessing import Imputer\n",
    "imp=Imputer(missing_values=\"NaN\", strategy=\"mean\" )\n",
    "data[\"age\"]=imp.fit_transform(data[[\"age\"]]).ravel()\n",
    "data[\"monthly_installment\"]=imp.fit_transform(data[[\"monthly_installment\"]]).ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorical missing values\n",
    "import numpy \n",
    "from sklearn.base import TransformerMixin\n",
    "class SeriesImputer(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        \"\"\"Impute missing values.\n",
    "        If the Series is of dtype Object, then impute with the most frequent object.\n",
    "        If the Series is not of dtype Object, then impute with the mean.  \n",
    "        \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        if   X.dtype == numpy.dtype('O'): self.fill = X.value_counts().index[0]\n",
    "        else                            : self.fill = X.mean()\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "       return X.fillna(self.fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a  = SeriesImputer()   # Initialize the imputer\n",
    "\n",
    "#filling with modus for edu code\n",
    "a.fit(data[\"edu_code\"])              # Fit the imputer\n",
    "newdata = a.transform(data[\"edu_code\"])   # Get a new series\n",
    "data[\"edu_code\"]=newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29096, 14)\n"
     ]
    }
   ],
   "source": [
    "#Remove rows with other missing values\n",
    "data.dropna(inplace=True)\n",
    "# summarize the number of rows and columns in the dataset\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 29096 entries, 0 to 29100\n",
      "Data columns (total 14 columns):\n",
      "durasi                 29096 non-null int64\n",
      "interest               29096 non-null float64\n",
      "net_income             29096 non-null float64\n",
      "additional_income      29096 non-null float64\n",
      "plafon                 29096 non-null float64\n",
      "biz_ownership          29096 non-null int64\n",
      "loyal_customer         29096 non-null int64\n",
      "age                    29096 non-null float64\n",
      "marriage_status        29096 non-null object\n",
      "gender                 29096 non-null object\n",
      "edu_code               29096 non-null object\n",
      "job_code               29096 non-null object\n",
      "monthly_installment    29096 non-null float64\n",
      "Good_Not_Good          29096 non-null int64\n",
      "dtypes: float64(6), int64(4), object(4)\n",
      "memory usage: 3.3+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    27679\n",
       "0     1417\n",
       "Name: good, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Class distribution\n",
    "data=data.rename(columns={'Good_Not_Good':'good'})\n",
    "data.good.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['durasi',\n",
       " 'interest',\n",
       " 'net_income',\n",
       " 'additional_income',\n",
       " 'plafon',\n",
       " 'biz_ownership',\n",
       " 'loyal_customer',\n",
       " 'age',\n",
       " 'marriage_status',\n",
       " 'gender',\n",
       " 'edu_code',\n",
       " 'job_code',\n",
       " 'monthly_installment']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List predictors\n",
    "X_features = list(data.columns)\n",
    "X_features.remove('good')\n",
    "X_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable output / class\n",
    "Y_feature=data['good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['durasi',\n",
       " 'interest',\n",
       " 'net_income',\n",
       " 'additional_income',\n",
       " 'plafon',\n",
       " 'biz_ownership',\n",
       " 'loyal_customer',\n",
       " 'age',\n",
       " 'monthly_installment',\n",
       " 'marriage_status_B',\n",
       " 'marriage_status_D',\n",
       " 'marriage_status_J',\n",
       " 'marriage_status_K',\n",
       " 'gender_F',\n",
       " 'gender_M',\n",
       " 'edu_code_S1',\n",
       " 'edu_code_S2',\n",
       " 'edu_code_S3',\n",
       " 'edu_code_SD',\n",
       " 'edu_code_SM',\n",
       " 'edu_code_SU',\n",
       " 'job_code_ADMI',\n",
       " 'job_code_AKUN',\n",
       " 'job_code_BUMN',\n",
       " 'job_code_DAGA',\n",
       " 'job_code_DKTR',\n",
       " 'job_code_EXEC',\n",
       " 'job_code_GURU',\n",
       " 'job_code_GUSW',\n",
       " 'job_code_IBRT',\n",
       " 'job_code_KOMP',\n",
       " 'job_code_KONS',\n",
       " 'job_code_MAHA',\n",
       " 'job_code_MILD',\n",
       " 'job_code_MILL',\n",
       " 'job_code_MILP',\n",
       " 'job_code_MILU',\n",
       " 'job_code_PELA',\n",
       " 'job_code_PEMI',\n",
       " 'job_code_PENG',\n",
       " 'job_code_PENS',\n",
       " 'job_code_PGCR',\n",
       " 'job_code_PNSI',\n",
       " 'job_code_PROD',\n",
       " 'job_code_PROF',\n",
       " 'job_code_RISE',\n",
       " 'job_code_SALE',\n",
       " 'job_code_SENI',\n",
       " 'job_code_SERV',\n",
       " 'job_code_SWAS',\n",
       " 'job_code_TECH',\n",
       " 'job_code_WIRA',\n",
       " 'job_code_ZZZZZ']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#One-hot-encoding : listing all categorical columns\n",
    "credit_df_complete = pd.get_dummies(data[X_features])\n",
    "list(credit_df_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set predictors and target\n",
    "predictors = credit_df_complete\n",
    "target = Y_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>durasi</th>\n",
       "      <th>interest</th>\n",
       "      <th>net_income</th>\n",
       "      <th>additional_income</th>\n",
       "      <th>plafon</th>\n",
       "      <th>biz_ownership</th>\n",
       "      <th>loyal_customer</th>\n",
       "      <th>age</th>\n",
       "      <th>monthly_installment</th>\n",
       "      <th>marriage_status_B</th>\n",
       "      <th>...</th>\n",
       "      <th>job_code_PROD</th>\n",
       "      <th>job_code_PROF</th>\n",
       "      <th>job_code_RISE</th>\n",
       "      <th>job_code_SALE</th>\n",
       "      <th>job_code_SENI</th>\n",
       "      <th>job_code_SERV</th>\n",
       "      <th>job_code_SWAS</th>\n",
       "      <th>job_code_TECH</th>\n",
       "      <th>job_code_WIRA</th>\n",
       "      <th>job_code_ZZZZZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.871000e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15000000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>778700.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.050000e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50000000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2683300.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.100000e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5000000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>270800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.590000e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15000000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1013300.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.130000e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35000000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>3336700.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29096</th>\n",
       "      <td>24</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.650000e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>536700.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29097</th>\n",
       "      <td>24</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.290000e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15000000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>778700.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29098</th>\n",
       "      <td>36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.150000e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30000000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1193300.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29099</th>\n",
       "      <td>6</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.921000e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5000000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>5542700.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29100</th>\n",
       "      <td>24</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.000000e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25000000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1341700.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29096 rows Ã— 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       durasi  interest    net_income  additional_income      plafon  \\\n",
       "0          24       8.0  2.871000e+10                0.0  15000000.0   \n",
       "1          24       8.0  5.050000e+10                0.0  50000000.0   \n",
       "2          24       8.0  1.100000e+10                0.0   5000000.0   \n",
       "3          18       8.0  1.590000e+10                0.0  15000000.0   \n",
       "4          12       8.0  5.130000e+11                0.0  35000000.0   \n",
       "...       ...       ...           ...                ...         ...   \n",
       "29096      24       8.0  1.650000e+10                0.0  10000000.0   \n",
       "29097      24       8.0  1.290000e+10                0.0  15000000.0   \n",
       "29098      36       8.0  9.150000e+09                0.0  30000000.0   \n",
       "29099       6       8.0  2.921000e+10                0.0   5000000.0   \n",
       "29100      24       8.0  6.000000e+10                0.0  25000000.0   \n",
       "\n",
       "       biz_ownership  loyal_customer   age  monthly_installment  \\\n",
       "0                  0               0  54.0             778700.0   \n",
       "1                  0               0  63.0            2683300.0   \n",
       "2                  1               0  50.0             270800.0   \n",
       "3                  1               0  50.0            1013300.0   \n",
       "4                  0               0  52.0            3336700.0   \n",
       "...              ...             ...   ...                  ...   \n",
       "29096              0               0  55.0             536700.0   \n",
       "29097              1               0  36.0             778700.0   \n",
       "29098              0               0  48.0            1193300.0   \n",
       "29099              0               0  36.0            5542700.0   \n",
       "29100              1               0  64.0            1341700.0   \n",
       "\n",
       "       marriage_status_B  ...  job_code_PROD  job_code_PROF  job_code_RISE  \\\n",
       "0                      1  ...              0              0              0   \n",
       "1                      1  ...              0              0              0   \n",
       "2                      1  ...              0              0              0   \n",
       "3                      1  ...              0              0              0   \n",
       "4                      1  ...              0              0              0   \n",
       "...                  ...  ...            ...            ...            ...   \n",
       "29096                  0  ...              0              0              0   \n",
       "29097                  0  ...              0              0              0   \n",
       "29098                  0  ...              0              0              0   \n",
       "29099                  0  ...              0              0              0   \n",
       "29100                  0  ...              0              0              0   \n",
       "\n",
       "       job_code_SALE  job_code_SENI  job_code_SERV  job_code_SWAS  \\\n",
       "0                  0              0              0              0   \n",
       "1                  0              0              0              0   \n",
       "2                  0              0              0              0   \n",
       "3                  0              0              0              0   \n",
       "4                  0              0              0              0   \n",
       "...              ...            ...            ...            ...   \n",
       "29096              0              0              0              0   \n",
       "29097              0              0              0              0   \n",
       "29098              0              0              0              0   \n",
       "29099              0              0              0              0   \n",
       "29100              0              0              0              0   \n",
       "\n",
       "       job_code_TECH  job_code_WIRA  job_code_ZZZZZ  \n",
       "0                  0              0               0  \n",
       "1                  0              0               0  \n",
       "2                  0              0               0  \n",
       "3                  0              0               0  \n",
       "4                  0              0               0  \n",
       "...              ...            ...             ...  \n",
       "29096              0              0               1  \n",
       "29097              0              0               1  \n",
       "29098              0              0               1  \n",
       "29099              0              0               1  \n",
       "29100              0              0               1  \n",
       "\n",
       "[29096 rows x 53 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#label encoding\n",
    "def dummyEncode(df):\n",
    "        columnsToEncode = list(df.select_dtypes(include=['category','object']))\n",
    "        le = LabelEncoder()\n",
    "        for feature in columnsToEncode:\n",
    "            try:\n",
    "                df[feature] = le.fit_transform(df[feature])\n",
    "            except:\n",
    "                print('Error encoding '+feature)\n",
    "        return df\n",
    "\n",
    "dummyEncode(predictors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Split training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictors,target,stratify=target,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape Counter({1: 22142, 0: 22142})\n"
     ]
    }
   ],
   "source": [
    "##Sampling\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "sampletech = RandomOverSampler(random_state=0)\n",
    "X_resampled, y_resampled = sampletech.fit_sample(X_train,\n",
    "y_train)\n",
    "from collections import Counter\n",
    "print('Resampled dataset shape {}'.format(Counter(y_resampled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "# Separate majority and minority classes\n",
    "df_majority = data[X_train.good==1s]\n",
    "df_minority = data[X_.good==0]\n",
    "\n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=26665,    # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    "df_minority_upsampled\n",
    "\n",
    "# Downsample minority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False,     # sample with replacement\n",
    "                                 n_samples=1404,    # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    "df_majority_downsampled\n",
    "\n",
    "import pandas as pd\n",
    "# Combine majority class with upsampled minority class\n",
    "#ups=[df_minority_upsampled,df_minority]\n",
    "dfnew=df_majority.append(df_minority_upsampled)\n",
    "dfnewz=df_minority.append(df_majority_downsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Normalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#Normalize data\n",
    "scaler = StandardScaler()\n",
    "# Fit only to the training data\n",
    "scaler.fit(X_train)\n",
    "# Now apply the transformations to the data:\n",
    "X_trainf = scaler.transform(X_resampled)\n",
    "X_testf = scaler.transform(X_test)\n",
    "y_train = y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69286869\n",
      "Iteration 2, loss = 0.68489179\n",
      "Iteration 3, loss = 0.68336554\n",
      "Iteration 4, loss = 0.68285728\n",
      "Iteration 5, loss = 0.68171499\n",
      "Iteration 6, loss = 0.68104150\n",
      "Iteration 7, loss = 0.67986242\n",
      "Iteration 8, loss = 0.67925871\n",
      "Iteration 9, loss = 0.67865801\n",
      "Iteration 10, loss = 0.67784799\n",
      "Iteration 11, loss = 0.67696239\n",
      "Iteration 12, loss = 0.67590879\n",
      "Iteration 13, loss = 0.67520367\n",
      "Iteration 14, loss = 0.67475149\n",
      "Iteration 15, loss = 0.67409883\n",
      "Iteration 16, loss = 0.67358829\n",
      "Iteration 17, loss = 0.67261805\n",
      "Iteration 18, loss = 0.67230687\n",
      "Iteration 19, loss = 0.67162161\n",
      "Iteration 20, loss = 0.67025749\n",
      "Iteration 21, loss = 0.67004705\n",
      "Iteration 22, loss = 0.66877484\n",
      "Iteration 23, loss = 0.66846705\n",
      "Iteration 24, loss = 0.66764999\n",
      "Iteration 25, loss = 0.66686163\n",
      "Iteration 26, loss = 0.66637866\n",
      "Iteration 27, loss = 0.66548964\n",
      "Iteration 28, loss = 0.66465563\n",
      "Iteration 29, loss = 0.66421865\n",
      "Iteration 30, loss = 0.66323005\n",
      "Iteration 31, loss = 0.66307915\n",
      "Iteration 32, loss = 0.66185772\n",
      "Iteration 33, loss = 0.66074743\n",
      "Iteration 34, loss = 0.66043317\n",
      "Iteration 35, loss = 0.65968724\n",
      "Iteration 36, loss = 0.65821654\n",
      "Iteration 37, loss = 0.65753866\n",
      "Iteration 38, loss = 0.65671845\n",
      "Iteration 39, loss = 0.65504098\n",
      "Iteration 40, loss = 0.65428845\n",
      "Iteration 41, loss = 0.65150013\n",
      "Iteration 42, loss = 0.65010579\n",
      "Iteration 43, loss = 0.64818219\n",
      "Iteration 44, loss = 0.64707651\n",
      "Iteration 45, loss = 0.64625803\n",
      "Iteration 46, loss = 0.64373227\n",
      "Iteration 47, loss = 0.64249575\n",
      "Iteration 48, loss = 0.64011179\n",
      "Iteration 49, loss = 0.63917050\n",
      "Iteration 50, loss = 0.63628980\n",
      "Iteration 51, loss = 0.63521774\n",
      "Iteration 52, loss = 0.63242681\n",
      "Iteration 53, loss = 0.63097893\n",
      "Iteration 54, loss = 0.62877481\n",
      "Iteration 55, loss = 0.62567275\n",
      "Iteration 56, loss = 0.62404597\n",
      "Iteration 57, loss = 0.62306068\n",
      "Iteration 58, loss = 0.62099142\n",
      "Iteration 59, loss = 0.61744988\n",
      "Iteration 60, loss = 0.61431538\n",
      "Iteration 61, loss = 0.61254925\n",
      "Iteration 62, loss = 0.60913082\n",
      "Iteration 63, loss = 0.60732036\n",
      "Iteration 64, loss = 0.60492338\n",
      "Iteration 65, loss = 0.60173190\n",
      "Iteration 66, loss = 0.59850041\n",
      "Iteration 67, loss = 0.59556111\n",
      "Iteration 68, loss = 0.59447272\n",
      "Iteration 69, loss = 0.59133232\n",
      "Iteration 70, loss = 0.58719338\n",
      "Iteration 71, loss = 0.58467097\n",
      "Iteration 72, loss = 0.58253359\n",
      "Iteration 73, loss = 0.57916908\n",
      "Iteration 74, loss = 0.57679937\n",
      "Iteration 75, loss = 0.57491371\n",
      "Iteration 76, loss = 0.57269010\n",
      "Iteration 77, loss = 0.56856188\n",
      "Iteration 78, loss = 0.56594009\n",
      "Iteration 79, loss = 0.56433443\n",
      "Iteration 80, loss = 0.56064548\n",
      "Iteration 81, loss = 0.55846790\n",
      "Iteration 82, loss = 0.55542249\n",
      "Iteration 83, loss = 0.55363058\n",
      "Iteration 84, loss = 0.55040364\n",
      "Iteration 85, loss = 0.54815573\n",
      "Iteration 86, loss = 0.54627908\n",
      "Iteration 87, loss = 0.54379943\n",
      "Iteration 88, loss = 0.54139047\n",
      "Iteration 89, loss = 0.53742930\n",
      "Iteration 90, loss = 0.53584348\n",
      "Iteration 91, loss = 0.53197181\n",
      "Iteration 92, loss = 0.53067595\n",
      "Iteration 93, loss = 0.52701797\n",
      "Iteration 94, loss = 0.52561199\n",
      "Iteration 95, loss = 0.52381341\n",
      "Iteration 96, loss = 0.52077489\n",
      "Iteration 97, loss = 0.51939822\n",
      "Iteration 98, loss = 0.51609503\n",
      "Iteration 99, loss = 0.51402872\n",
      "Iteration 100, loss = 0.51297601\n",
      "Iteration 101, loss = 0.50990062\n",
      "Iteration 102, loss = 0.50705517\n",
      "Iteration 103, loss = 0.50597590\n",
      "Iteration 104, loss = 0.50314257\n",
      "Iteration 105, loss = 0.49955989\n",
      "Iteration 106, loss = 0.49889997\n",
      "Iteration 107, loss = 0.49693265\n",
      "Iteration 108, loss = 0.49609324\n",
      "Iteration 109, loss = 0.49249709\n",
      "Iteration 110, loss = 0.49073563\n",
      "Iteration 111, loss = 0.48940994\n",
      "Iteration 112, loss = 0.48628110\n",
      "Iteration 113, loss = 0.48416341\n",
      "Iteration 114, loss = 0.48208630\n",
      "Iteration 115, loss = 0.48083884\n",
      "Iteration 116, loss = 0.47843499\n",
      "Iteration 117, loss = 0.47755165\n",
      "Iteration 118, loss = 0.47484983\n",
      "Iteration 119, loss = 0.47322655\n",
      "Iteration 120, loss = 0.47107502\n",
      "Iteration 121, loss = 0.47014272\n",
      "Iteration 122, loss = 0.46791486\n",
      "Iteration 123, loss = 0.46619280\n",
      "Iteration 124, loss = 0.46349247\n",
      "Iteration 125, loss = 0.46178411\n",
      "Iteration 126, loss = 0.46111874\n",
      "Iteration 127, loss = 0.45905590\n",
      "Iteration 128, loss = 0.45901592\n",
      "Iteration 129, loss = 0.45572466\n",
      "Iteration 130, loss = 0.45289332\n",
      "Iteration 131, loss = 0.45180789\n",
      "Iteration 132, loss = 0.45110763\n",
      "Iteration 133, loss = 0.45010400\n",
      "Iteration 134, loss = 0.44767006\n",
      "Iteration 135, loss = 0.44662400\n",
      "Iteration 136, loss = 0.44364803\n",
      "Iteration 137, loss = 0.44273900\n",
      "Iteration 138, loss = 0.44074612\n",
      "Iteration 139, loss = 0.43888388\n",
      "Iteration 140, loss = 0.43780050\n",
      "Iteration 141, loss = 0.43569917\n",
      "Iteration 142, loss = 0.43494636\n",
      "Iteration 143, loss = 0.43216508\n",
      "Iteration 144, loss = 0.43081773\n",
      "Iteration 145, loss = 0.43010821\n",
      "Iteration 146, loss = 0.42894272\n",
      "Iteration 147, loss = 0.42819274\n",
      "Iteration 148, loss = 0.42573261\n",
      "Iteration 149, loss = 0.42481711\n",
      "Iteration 150, loss = 0.42238590\n",
      "Iteration 151, loss = 0.42082525\n",
      "Iteration 152, loss = 0.41982604\n",
      "Iteration 153, loss = 0.41886697\n",
      "Iteration 154, loss = 0.41718407\n",
      "Iteration 155, loss = 0.41448191\n",
      "Iteration 156, loss = 0.41537013\n",
      "Iteration 157, loss = 0.41306941\n",
      "Iteration 158, loss = 0.41163727\n",
      "Iteration 159, loss = 0.41053368\n",
      "Iteration 160, loss = 0.40856492\n",
      "Iteration 161, loss = 0.40736864\n",
      "Iteration 162, loss = 0.40606533\n",
      "Iteration 163, loss = 0.40512534\n",
      "Iteration 164, loss = 0.40399184\n",
      "Iteration 165, loss = 0.40211003\n",
      "Iteration 166, loss = 0.40224762\n",
      "Iteration 167, loss = 0.39976053\n",
      "Iteration 168, loss = 0.39837250\n",
      "Iteration 169, loss = 0.39721829\n",
      "Iteration 170, loss = 0.39705463\n",
      "Iteration 171, loss = 0.39468051\n",
      "Iteration 172, loss = 0.39311431\n",
      "Iteration 173, loss = 0.39277197\n",
      "Iteration 174, loss = 0.39002674\n",
      "Iteration 175, loss = 0.39104706\n",
      "Iteration 176, loss = 0.38869133\n",
      "Iteration 177, loss = 0.38826345\n",
      "Iteration 178, loss = 0.38485518\n",
      "Iteration 179, loss = 0.38485239\n",
      "Iteration 180, loss = 0.38330301\n",
      "Iteration 181, loss = 0.38143051\n",
      "Iteration 182, loss = 0.38067947\n",
      "Iteration 183, loss = 0.38055489\n",
      "Iteration 184, loss = 0.37828106\n",
      "Iteration 185, loss = 0.37709132\n",
      "Iteration 186, loss = 0.37636191\n",
      "Iteration 187, loss = 0.37418758\n",
      "Iteration 188, loss = 0.37457893\n",
      "Iteration 189, loss = 0.37361140\n",
      "Iteration 190, loss = 0.37193388\n",
      "Iteration 191, loss = 0.37057726\n",
      "Iteration 192, loss = 0.37000752\n",
      "Iteration 193, loss = 0.36855639\n",
      "Iteration 194, loss = 0.36781332\n",
      "Iteration 195, loss = 0.36570171\n",
      "Iteration 196, loss = 0.36533410\n",
      "Iteration 197, loss = 0.36461973\n",
      "Iteration 198, loss = 0.36379888\n",
      "Iteration 199, loss = 0.36117269\n",
      "Iteration 200, loss = 0.36237512\n",
      "Iteration 201, loss = 0.35980472\n",
      "Iteration 202, loss = 0.35889539\n",
      "Iteration 203, loss = 0.35732474\n",
      "Iteration 204, loss = 0.35807750\n",
      "Iteration 205, loss = 0.35547714\n",
      "Iteration 206, loss = 0.35433717\n",
      "Iteration 207, loss = 0.35291299\n",
      "Iteration 208, loss = 0.35264423\n",
      "Iteration 209, loss = 0.35196083\n",
      "Iteration 210, loss = 0.35098687\n",
      "Iteration 211, loss = 0.34941270\n",
      "Iteration 212, loss = 0.34782080\n",
      "Iteration 213, loss = 0.34858888\n",
      "Iteration 214, loss = 0.34689869\n",
      "Iteration 215, loss = 0.34506165\n",
      "Iteration 216, loss = 0.34508559\n",
      "Iteration 217, loss = 0.34325108\n",
      "Iteration 218, loss = 0.34243258\n",
      "Iteration 219, loss = 0.34170472\n",
      "Iteration 220, loss = 0.34187592\n",
      "Iteration 221, loss = 0.33967390\n",
      "Iteration 222, loss = 0.33842166\n",
      "Iteration 223, loss = 0.33870420\n",
      "Iteration 224, loss = 0.33738064\n",
      "Iteration 225, loss = 0.33630983\n",
      "Iteration 226, loss = 0.33500171\n",
      "Iteration 227, loss = 0.33456820\n",
      "Iteration 228, loss = 0.33385235\n",
      "Iteration 229, loss = 0.33287741\n",
      "Iteration 230, loss = 0.33118471\n",
      "Iteration 231, loss = 0.33039439\n",
      "Iteration 232, loss = 0.33107791\n",
      "Iteration 233, loss = 0.32910101\n",
      "Iteration 234, loss = 0.32786091\n",
      "Iteration 235, loss = 0.32725128\n",
      "Iteration 236, loss = 0.32651161\n",
      "Iteration 237, loss = 0.32583648\n",
      "Iteration 238, loss = 0.32552066\n",
      "Iteration 239, loss = 0.32460389\n",
      "Iteration 240, loss = 0.32334657\n",
      "Iteration 241, loss = 0.32267174\n",
      "Iteration 242, loss = 0.32055907\n",
      "Iteration 243, loss = 0.32077363\n",
      "Iteration 244, loss = 0.32012786\n",
      "Iteration 245, loss = 0.31968018\n",
      "Iteration 246, loss = 0.31853290\n",
      "Iteration 247, loss = 0.31733859\n",
      "Iteration 248, loss = 0.31745968\n",
      "Iteration 249, loss = 0.31564560\n",
      "Iteration 250, loss = 0.31499765\n",
      "Iteration 251, loss = 0.31328558\n",
      "Iteration 252, loss = 0.31378949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.31224728\n",
      "Iteration 254, loss = 0.31185492\n",
      "Iteration 255, loss = 0.31174661\n",
      "Iteration 256, loss = 0.31061388\n",
      "Iteration 257, loss = 0.30986993\n",
      "Iteration 258, loss = 0.30877550\n",
      "Iteration 259, loss = 0.30798434\n",
      "Iteration 260, loss = 0.30911814\n",
      "Iteration 261, loss = 0.30660712\n",
      "Iteration 262, loss = 0.30532023\n",
      "Iteration 263, loss = 0.30552066\n",
      "Iteration 264, loss = 0.30274603\n",
      "Iteration 265, loss = 0.30384540\n",
      "Iteration 266, loss = 0.30287038\n",
      "Iteration 267, loss = 0.30127817\n",
      "Iteration 268, loss = 0.30083608\n",
      "Iteration 269, loss = 0.30101108\n",
      "Iteration 270, loss = 0.30020108\n",
      "Iteration 271, loss = 0.30037624\n",
      "Iteration 272, loss = 0.29923290\n",
      "Iteration 273, loss = 0.29825190\n",
      "Iteration 274, loss = 0.29832701\n",
      "Iteration 275, loss = 0.29608605\n",
      "Iteration 276, loss = 0.29474062\n",
      "Iteration 277, loss = 0.29610776\n",
      "Iteration 278, loss = 0.29434685\n",
      "Iteration 279, loss = 0.29319982\n",
      "Iteration 280, loss = 0.29359556\n",
      "Iteration 281, loss = 0.29262566\n",
      "Iteration 282, loss = 0.29167737\n",
      "Iteration 283, loss = 0.29226986\n",
      "Iteration 284, loss = 0.28986442\n",
      "Iteration 285, loss = 0.28822073\n",
      "Iteration 286, loss = 0.28925682\n",
      "Iteration 287, loss = 0.29053770\n",
      "Iteration 288, loss = 0.28872809\n",
      "Iteration 289, loss = 0.28730006\n",
      "Iteration 290, loss = 0.28678246\n",
      "Iteration 291, loss = 0.28736701\n",
      "Iteration 292, loss = 0.28522256\n",
      "Iteration 293, loss = 0.28505144\n",
      "Iteration 294, loss = 0.28446159\n",
      "Iteration 295, loss = 0.28307431\n",
      "Iteration 296, loss = 0.28385615\n",
      "Iteration 297, loss = 0.28191274\n",
      "Iteration 298, loss = 0.28140373\n",
      "Iteration 299, loss = 0.28069812\n",
      "Iteration 300, loss = 0.28001208\n",
      "Iteration 301, loss = 0.28007091\n",
      "Iteration 302, loss = 0.27851402\n",
      "Iteration 303, loss = 0.27723783\n",
      "Iteration 304, loss = 0.27827259\n",
      "Iteration 305, loss = 0.27752846\n",
      "Iteration 306, loss = 0.27670278\n",
      "Iteration 307, loss = 0.27577290\n",
      "Iteration 308, loss = 0.27590690\n",
      "Iteration 309, loss = 0.27552963\n",
      "Iteration 310, loss = 0.27572661\n",
      "Iteration 311, loss = 0.27439331\n",
      "Iteration 312, loss = 0.27562384\n",
      "Iteration 313, loss = 0.27544091\n",
      "Iteration 314, loss = 0.27314576\n",
      "Iteration 315, loss = 0.27307363\n",
      "Iteration 316, loss = 0.27172083\n",
      "Iteration 317, loss = 0.27187264\n",
      "Iteration 318, loss = 0.27081198\n",
      "Iteration 319, loss = 0.27073183\n",
      "Iteration 320, loss = 0.26998091\n",
      "Iteration 321, loss = 0.26831330\n",
      "Iteration 322, loss = 0.26945028\n",
      "Iteration 323, loss = 0.26916817\n",
      "Iteration 324, loss = 0.26818508\n",
      "Iteration 325, loss = 0.26711963\n",
      "Iteration 326, loss = 0.26714938\n",
      "Iteration 327, loss = 0.26665429\n",
      "Iteration 328, loss = 0.26754122\n",
      "Iteration 329, loss = 0.26520823\n",
      "Iteration 330, loss = 0.26388959\n",
      "Iteration 331, loss = 0.26495806\n",
      "Iteration 332, loss = 0.26402881\n",
      "Iteration 333, loss = 0.26377344\n",
      "Iteration 334, loss = 0.26356738\n",
      "Iteration 335, loss = 0.26270485\n",
      "Iteration 336, loss = 0.26223403\n",
      "Iteration 337, loss = 0.26108074\n",
      "Iteration 338, loss = 0.26185802\n",
      "Iteration 339, loss = 0.26092017\n",
      "Iteration 340, loss = 0.26134470\n",
      "Iteration 341, loss = 0.25943402\n",
      "Iteration 342, loss = 0.25982945\n",
      "Iteration 343, loss = 0.25884734\n",
      "Iteration 344, loss = 0.25852677\n",
      "Iteration 345, loss = 0.25716594\n",
      "Iteration 346, loss = 0.25712658\n",
      "Iteration 347, loss = 0.25691258\n",
      "Iteration 348, loss = 0.25816420\n",
      "Iteration 349, loss = 0.25694591\n",
      "Iteration 350, loss = 0.25570463\n",
      "Iteration 351, loss = 0.25547914\n",
      "Iteration 352, loss = 0.25565892\n",
      "Iteration 353, loss = 0.25379807\n",
      "Iteration 354, loss = 0.25530378\n",
      "Iteration 355, loss = 0.25383525\n",
      "Iteration 356, loss = 0.25446007\n",
      "Iteration 357, loss = 0.25257426\n",
      "Iteration 358, loss = 0.25139337\n",
      "Iteration 359, loss = 0.25159337\n",
      "Iteration 360, loss = 0.25095806\n",
      "Iteration 361, loss = 0.25116590\n",
      "Iteration 362, loss = 0.25030881\n",
      "Iteration 363, loss = 0.24920795\n",
      "Iteration 364, loss = 0.25109897\n",
      "Iteration 365, loss = 0.24989368\n",
      "Iteration 366, loss = 0.24837914\n",
      "Iteration 367, loss = 0.24741439\n",
      "Iteration 368, loss = 0.24806644\n",
      "Iteration 369, loss = 0.24764887\n",
      "Iteration 370, loss = 0.24637914\n",
      "Iteration 371, loss = 0.24686011\n",
      "Iteration 372, loss = 0.24760826\n",
      "Iteration 373, loss = 0.24527967\n",
      "Iteration 374, loss = 0.24617314\n",
      "Iteration 375, loss = 0.24500726\n",
      "Iteration 376, loss = 0.24482273\n",
      "Iteration 377, loss = 0.24443207\n",
      "Iteration 378, loss = 0.24411410\n",
      "Iteration 379, loss = 0.24347139\n",
      "Iteration 380, loss = 0.24343829\n",
      "Iteration 381, loss = 0.24274173\n",
      "Iteration 382, loss = 0.24238838\n",
      "Iteration 383, loss = 0.24249257\n",
      "Iteration 384, loss = 0.24277173\n",
      "Iteration 385, loss = 0.24071591\n",
      "Iteration 386, loss = 0.24178666\n",
      "Iteration 387, loss = 0.24029613\n",
      "Iteration 388, loss = 0.23921605\n",
      "Iteration 389, loss = 0.23997319\n",
      "Iteration 390, loss = 0.23926162\n",
      "Iteration 391, loss = 0.23938568\n",
      "Iteration 392, loss = 0.23746945\n",
      "Iteration 393, loss = 0.23768881\n",
      "Iteration 394, loss = 0.23738880\n",
      "Iteration 395, loss = 0.23774439\n",
      "Iteration 396, loss = 0.23667774\n",
      "Iteration 397, loss = 0.23631055\n",
      "Iteration 398, loss = 0.23655615\n",
      "Iteration 399, loss = 0.23612559\n",
      "Iteration 400, loss = 0.23504859\n",
      "Iteration 401, loss = 0.23502788\n",
      "Iteration 402, loss = 0.23529763\n",
      "Iteration 403, loss = 0.23344469\n",
      "Iteration 404, loss = 0.23457603\n",
      "Iteration 405, loss = 0.23324150\n",
      "Iteration 406, loss = 0.23212362\n",
      "Iteration 407, loss = 0.23311724\n",
      "Iteration 408, loss = 0.23159441\n",
      "Iteration 409, loss = 0.23326380\n",
      "Iteration 410, loss = 0.23082906\n",
      "Iteration 411, loss = 0.23091067\n",
      "Iteration 412, loss = 0.23093200\n",
      "Iteration 413, loss = 0.23084611\n",
      "Iteration 414, loss = 0.22948696\n",
      "Iteration 415, loss = 0.22997093\n",
      "Iteration 416, loss = 0.22902403\n",
      "Iteration 417, loss = 0.22906953\n",
      "Iteration 418, loss = 0.22872617\n",
      "Iteration 419, loss = 0.22747799\n",
      "Iteration 420, loss = 0.22840422\n",
      "Iteration 421, loss = 0.22739004\n",
      "Iteration 422, loss = 0.22690391\n",
      "Iteration 423, loss = 0.22567229\n",
      "Iteration 424, loss = 0.22669336\n",
      "Iteration 425, loss = 0.22527405\n",
      "Iteration 426, loss = 0.22527230\n",
      "Iteration 427, loss = 0.22524734\n",
      "Iteration 428, loss = 0.22585711\n",
      "Iteration 429, loss = 0.22486094\n",
      "Iteration 430, loss = 0.22378714\n",
      "Iteration 431, loss = 0.22456219\n",
      "Iteration 432, loss = 0.22485873\n",
      "Iteration 433, loss = 0.22328010\n",
      "Iteration 434, loss = 0.22348678\n",
      "Iteration 435, loss = 0.22286218\n",
      "Iteration 436, loss = 0.22257400\n",
      "Iteration 437, loss = 0.22184166\n",
      "Iteration 438, loss = 0.22223662\n",
      "Iteration 439, loss = 0.22132302\n",
      "Iteration 440, loss = 0.22107432\n",
      "Iteration 441, loss = 0.22154976\n",
      "Iteration 442, loss = 0.21944682\n",
      "Iteration 443, loss = 0.21933123\n",
      "Iteration 444, loss = 0.21983272\n",
      "Iteration 445, loss = 0.21956217\n",
      "Iteration 446, loss = 0.21862133\n",
      "Iteration 447, loss = 0.21853580\n",
      "Iteration 448, loss = 0.21919851\n",
      "Iteration 449, loss = 0.21862268\n",
      "Iteration 450, loss = 0.21728662\n",
      "Iteration 451, loss = 0.21777718\n",
      "Iteration 452, loss = 0.21695406\n",
      "Iteration 453, loss = 0.21598472\n",
      "Iteration 454, loss = 0.21695837\n",
      "Iteration 455, loss = 0.21600737\n",
      "Iteration 456, loss = 0.21522542\n",
      "Iteration 457, loss = 0.21584939\n",
      "Iteration 458, loss = 0.21498244\n",
      "Iteration 459, loss = 0.21507694\n",
      "Iteration 460, loss = 0.21460760\n",
      "Iteration 461, loss = 0.21529970\n",
      "Iteration 462, loss = 0.21379606\n",
      "Iteration 463, loss = 0.21406610\n",
      "Iteration 464, loss = 0.21398572\n",
      "Iteration 465, loss = 0.21337398\n",
      "Iteration 466, loss = 0.21387876\n",
      "Iteration 467, loss = 0.21209164\n",
      "Iteration 468, loss = 0.21322400\n",
      "Iteration 469, loss = 0.21169353\n",
      "Iteration 470, loss = 0.21112727\n",
      "Iteration 471, loss = 0.21173282\n",
      "Iteration 472, loss = 0.21148583\n",
      "Iteration 473, loss = 0.21122882\n",
      "Iteration 474, loss = 0.21038875\n",
      "Iteration 475, loss = 0.21032701\n",
      "Iteration 476, loss = 0.20911735\n",
      "Iteration 477, loss = 0.21009750\n",
      "Iteration 478, loss = 0.20945121\n",
      "Iteration 479, loss = 0.20900763\n",
      "Iteration 480, loss = 0.20868640\n",
      "Iteration 481, loss = 0.20960514\n",
      "Iteration 482, loss = 0.20808827\n",
      "Iteration 483, loss = 0.20763257\n",
      "Iteration 484, loss = 0.20776728\n",
      "Iteration 485, loss = 0.20796741\n",
      "Iteration 486, loss = 0.20761243\n",
      "Iteration 487, loss = 0.20701933\n",
      "Iteration 488, loss = 0.20666718\n",
      "Iteration 489, loss = 0.20559877\n",
      "Iteration 490, loss = 0.20655904\n",
      "Iteration 491, loss = 0.20527831\n",
      "Iteration 492, loss = 0.20603547\n",
      "Iteration 493, loss = 0.20557604\n",
      "Iteration 494, loss = 0.20503282\n",
      "Iteration 495, loss = 0.20420585\n",
      "Iteration 496, loss = 0.20421620\n",
      "Iteration 497, loss = 0.20528874\n",
      "Iteration 498, loss = 0.20430459\n",
      "Iteration 499, loss = 0.20433986\n",
      "Iteration 500, loss = 0.20443684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69257359\n",
      "Iteration 2, loss = 0.68531849\n",
      "Iteration 3, loss = 0.68431978\n",
      "Iteration 4, loss = 0.68364146\n",
      "Iteration 5, loss = 0.68290524\n",
      "Iteration 6, loss = 0.68186211\n",
      "Iteration 7, loss = 0.68046507\n",
      "Iteration 8, loss = 0.67947670\n",
      "Iteration 9, loss = 0.67899256\n",
      "Iteration 10, loss = 0.67827845\n",
      "Iteration 11, loss = 0.67692381\n",
      "Iteration 12, loss = 0.67664785\n",
      "Iteration 13, loss = 0.67577904\n",
      "Iteration 14, loss = 0.67502177\n",
      "Iteration 15, loss = 0.67487587\n",
      "Iteration 16, loss = 0.67404903\n",
      "Iteration 17, loss = 0.67338579\n",
      "Iteration 18, loss = 0.67244465\n",
      "Iteration 19, loss = 0.67219233\n",
      "Iteration 20, loss = 0.67035142\n",
      "Iteration 21, loss = 0.66969990\n",
      "Iteration 22, loss = 0.66907482\n",
      "Iteration 23, loss = 0.66799539\n",
      "Iteration 24, loss = 0.66687937\n",
      "Iteration 25, loss = 0.66616861\n",
      "Iteration 26, loss = 0.66577764\n",
      "Iteration 27, loss = 0.66473609\n",
      "Iteration 28, loss = 0.66370290\n",
      "Iteration 29, loss = 0.66295335\n",
      "Iteration 30, loss = 0.66128396\n",
      "Iteration 31, loss = 0.66094686\n",
      "Iteration 32, loss = 0.65985847\n",
      "Iteration 33, loss = 0.65874933\n",
      "Iteration 34, loss = 0.65774157\n",
      "Iteration 35, loss = 0.65638967\n",
      "Iteration 36, loss = 0.65414057\n",
      "Iteration 37, loss = 0.65349593\n",
      "Iteration 38, loss = 0.65223584\n",
      "Iteration 39, loss = 0.65093426\n",
      "Iteration 40, loss = 0.65006349\n",
      "Iteration 41, loss = 0.64768228\n",
      "Iteration 42, loss = 0.64689488\n",
      "Iteration 43, loss = 0.64547400\n",
      "Iteration 44, loss = 0.64426172\n",
      "Iteration 45, loss = 0.64255637\n",
      "Iteration 46, loss = 0.64078675\n",
      "Iteration 47, loss = 0.63955034\n",
      "Iteration 48, loss = 0.63735637\n",
      "Iteration 49, loss = 0.63588734\n",
      "Iteration 50, loss = 0.63376236\n",
      "Iteration 51, loss = 0.63196139\n",
      "Iteration 52, loss = 0.62952290\n",
      "Iteration 53, loss = 0.62759324\n",
      "Iteration 54, loss = 0.62535160\n",
      "Iteration 55, loss = 0.62344009\n",
      "Iteration 56, loss = 0.62091283\n",
      "Iteration 57, loss = 0.61943201\n",
      "Iteration 58, loss = 0.61659415\n",
      "Iteration 59, loss = 0.61480589\n",
      "Iteration 60, loss = 0.61184492\n",
      "Iteration 61, loss = 0.60991000\n",
      "Iteration 62, loss = 0.60698717\n",
      "Iteration 63, loss = 0.60428752\n",
      "Iteration 64, loss = 0.60183025\n",
      "Iteration 65, loss = 0.59979404\n",
      "Iteration 66, loss = 0.59628974\n",
      "Iteration 67, loss = 0.59448509\n",
      "Iteration 68, loss = 0.59127659\n",
      "Iteration 69, loss = 0.58870566\n",
      "Iteration 70, loss = 0.58594404\n",
      "Iteration 71, loss = 0.58320801\n",
      "Iteration 72, loss = 0.58053819\n",
      "Iteration 73, loss = 0.57779223\n",
      "Iteration 74, loss = 0.57603144\n",
      "Iteration 75, loss = 0.57386955\n",
      "Iteration 76, loss = 0.57143571\n",
      "Iteration 77, loss = 0.56784276\n",
      "Iteration 78, loss = 0.56538515\n",
      "Iteration 79, loss = 0.56368403\n",
      "Iteration 80, loss = 0.56132738\n",
      "Iteration 81, loss = 0.55897910\n",
      "Iteration 82, loss = 0.55678504\n",
      "Iteration 83, loss = 0.55335983\n",
      "Iteration 84, loss = 0.55122240\n",
      "Iteration 85, loss = 0.54899407\n",
      "Iteration 86, loss = 0.54645681\n",
      "Iteration 87, loss = 0.54426500\n",
      "Iteration 88, loss = 0.54253698\n",
      "Iteration 89, loss = 0.53940920\n",
      "Iteration 90, loss = 0.53732986\n",
      "Iteration 91, loss = 0.53530093\n",
      "Iteration 92, loss = 0.53276764\n",
      "Iteration 93, loss = 0.53058454\n",
      "Iteration 94, loss = 0.52941637\n",
      "Iteration 95, loss = 0.52717972\n",
      "Iteration 96, loss = 0.52407233\n",
      "Iteration 97, loss = 0.52265939\n",
      "Iteration 98, loss = 0.52001067\n",
      "Iteration 99, loss = 0.51815599\n",
      "Iteration 100, loss = 0.51596249\n",
      "Iteration 101, loss = 0.51413323\n",
      "Iteration 102, loss = 0.51158480\n",
      "Iteration 103, loss = 0.51063273\n",
      "Iteration 104, loss = 0.50823252\n",
      "Iteration 105, loss = 0.50618707\n",
      "Iteration 106, loss = 0.50420350\n",
      "Iteration 107, loss = 0.50282663\n",
      "Iteration 108, loss = 0.50040217\n",
      "Iteration 109, loss = 0.49944647\n",
      "Iteration 110, loss = 0.49545251\n",
      "Iteration 111, loss = 0.49375520\n",
      "Iteration 112, loss = 0.49179876\n",
      "Iteration 113, loss = 0.49108589\n",
      "Iteration 114, loss = 0.48862491\n",
      "Iteration 115, loss = 0.48607292\n",
      "Iteration 116, loss = 0.48535935\n",
      "Iteration 117, loss = 0.48246252\n",
      "Iteration 118, loss = 0.48213790\n",
      "Iteration 119, loss = 0.47944054\n",
      "Iteration 120, loss = 0.47749538\n",
      "Iteration 121, loss = 0.47692595\n",
      "Iteration 122, loss = 0.47539701\n",
      "Iteration 123, loss = 0.47184698\n",
      "Iteration 124, loss = 0.47094404\n",
      "Iteration 125, loss = 0.46901557\n",
      "Iteration 126, loss = 0.46858964\n",
      "Iteration 127, loss = 0.46550903\n",
      "Iteration 128, loss = 0.46373804\n",
      "Iteration 129, loss = 0.46223599\n",
      "Iteration 130, loss = 0.46111094\n",
      "Iteration 131, loss = 0.45993020\n",
      "Iteration 132, loss = 0.45800557\n",
      "Iteration 133, loss = 0.45640135\n",
      "Iteration 134, loss = 0.45484388\n",
      "Iteration 135, loss = 0.45430514\n",
      "Iteration 136, loss = 0.45186283\n",
      "Iteration 137, loss = 0.45078999\n",
      "Iteration 138, loss = 0.44921355\n",
      "Iteration 139, loss = 0.44646670\n",
      "Iteration 140, loss = 0.44614218\n",
      "Iteration 141, loss = 0.44556645\n",
      "Iteration 142, loss = 0.44284570\n",
      "Iteration 143, loss = 0.44192065\n",
      "Iteration 144, loss = 0.44167324\n",
      "Iteration 145, loss = 0.43940073\n",
      "Iteration 146, loss = 0.43836001\n",
      "Iteration 147, loss = 0.43784358\n",
      "Iteration 148, loss = 0.43520387\n",
      "Iteration 149, loss = 0.43445957\n",
      "Iteration 150, loss = 0.43215949\n",
      "Iteration 151, loss = 0.43028442\n",
      "Iteration 152, loss = 0.42957457\n",
      "Iteration 153, loss = 0.42932838\n",
      "Iteration 154, loss = 0.42682261\n",
      "Iteration 155, loss = 0.42589395\n",
      "Iteration 156, loss = 0.42459424\n",
      "Iteration 157, loss = 0.42346138\n",
      "Iteration 158, loss = 0.42114098\n",
      "Iteration 159, loss = 0.42004135\n",
      "Iteration 160, loss = 0.42046656\n",
      "Iteration 161, loss = 0.41807450\n",
      "Iteration 162, loss = 0.41722080\n",
      "Iteration 163, loss = 0.41699815\n",
      "Iteration 164, loss = 0.41555718\n",
      "Iteration 165, loss = 0.41419303\n",
      "Iteration 166, loss = 0.41359795\n",
      "Iteration 167, loss = 0.41240153\n",
      "Iteration 168, loss = 0.41072649\n",
      "Iteration 169, loss = 0.41000350\n",
      "Iteration 170, loss = 0.40916235\n",
      "Iteration 171, loss = 0.40673690\n",
      "Iteration 172, loss = 0.40604867\n",
      "Iteration 173, loss = 0.40508102\n",
      "Iteration 174, loss = 0.40398846\n",
      "Iteration 175, loss = 0.40361586\n",
      "Iteration 176, loss = 0.40165981\n",
      "Iteration 177, loss = 0.40050007\n",
      "Iteration 178, loss = 0.40115698\n",
      "Iteration 179, loss = 0.39848736\n",
      "Iteration 180, loss = 0.39730370\n",
      "Iteration 181, loss = 0.39630544\n",
      "Iteration 182, loss = 0.39567282\n",
      "Iteration 183, loss = 0.39530810\n",
      "Iteration 184, loss = 0.39421581\n",
      "Iteration 185, loss = 0.39253940\n",
      "Iteration 186, loss = 0.39101168\n",
      "Iteration 187, loss = 0.38969082\n",
      "Iteration 188, loss = 0.38958102\n",
      "Iteration 189, loss = 0.38831211\n",
      "Iteration 190, loss = 0.38830517\n",
      "Iteration 191, loss = 0.38653658\n",
      "Iteration 192, loss = 0.38532322\n",
      "Iteration 193, loss = 0.38420880\n",
      "Iteration 194, loss = 0.38343317\n",
      "Iteration 195, loss = 0.38287106\n",
      "Iteration 196, loss = 0.38186637\n",
      "Iteration 197, loss = 0.38001410\n",
      "Iteration 198, loss = 0.38021453\n",
      "Iteration 199, loss = 0.37852272\n",
      "Iteration 200, loss = 0.37871768\n",
      "Iteration 201, loss = 0.37741017\n",
      "Iteration 202, loss = 0.37563782\n",
      "Iteration 203, loss = 0.37550053\n",
      "Iteration 204, loss = 0.37497492\n",
      "Iteration 205, loss = 0.37367567\n",
      "Iteration 206, loss = 0.37309097\n",
      "Iteration 207, loss = 0.37146436\n",
      "Iteration 208, loss = 0.37195973\n",
      "Iteration 209, loss = 0.36947559\n",
      "Iteration 210, loss = 0.36881602\n",
      "Iteration 211, loss = 0.36827339\n",
      "Iteration 212, loss = 0.36680820\n",
      "Iteration 213, loss = 0.36600635\n",
      "Iteration 214, loss = 0.36577144\n",
      "Iteration 215, loss = 0.36476211\n",
      "Iteration 216, loss = 0.36397601\n",
      "Iteration 217, loss = 0.36197376\n",
      "Iteration 218, loss = 0.36300175\n",
      "Iteration 219, loss = 0.36105236\n",
      "Iteration 220, loss = 0.36069823\n",
      "Iteration 221, loss = 0.35951814\n",
      "Iteration 222, loss = 0.35802404\n",
      "Iteration 223, loss = 0.35741619\n",
      "Iteration 224, loss = 0.35656007\n",
      "Iteration 225, loss = 0.35607946\n",
      "Iteration 226, loss = 0.35452850\n",
      "Iteration 227, loss = 0.35460276\n",
      "Iteration 228, loss = 0.35221336\n",
      "Iteration 229, loss = 0.35216631\n",
      "Iteration 230, loss = 0.35191894\n",
      "Iteration 231, loss = 0.35044768\n",
      "Iteration 232, loss = 0.34981419\n",
      "Iteration 233, loss = 0.34913393\n",
      "Iteration 234, loss = 0.34903080\n",
      "Iteration 235, loss = 0.34676049\n",
      "Iteration 236, loss = 0.34756633\n",
      "Iteration 237, loss = 0.34519575\n",
      "Iteration 238, loss = 0.34545243\n",
      "Iteration 239, loss = 0.34438149\n",
      "Iteration 240, loss = 0.34374948\n",
      "Iteration 241, loss = 0.34265042\n",
      "Iteration 242, loss = 0.34179662\n",
      "Iteration 243, loss = 0.34311060\n",
      "Iteration 244, loss = 0.34109675\n",
      "Iteration 245, loss = 0.33974039\n",
      "Iteration 246, loss = 0.33976687\n",
      "Iteration 247, loss = 0.33894346\n",
      "Iteration 248, loss = 0.33691511\n",
      "Iteration 249, loss = 0.33686235\n",
      "Iteration 250, loss = 0.33600737\n",
      "Iteration 251, loss = 0.33621257\n",
      "Iteration 252, loss = 0.33420205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.33317131\n",
      "Iteration 254, loss = 0.33324045\n",
      "Iteration 255, loss = 0.33235722\n",
      "Iteration 256, loss = 0.33206037\n",
      "Iteration 257, loss = 0.33259811\n",
      "Iteration 258, loss = 0.33014747\n",
      "Iteration 259, loss = 0.33029777\n",
      "Iteration 260, loss = 0.32984191\n",
      "Iteration 261, loss = 0.32892855\n",
      "Iteration 262, loss = 0.32785554\n",
      "Iteration 263, loss = 0.32750857\n",
      "Iteration 264, loss = 0.32729153\n",
      "Iteration 265, loss = 0.32652742\n",
      "Iteration 266, loss = 0.32637704\n",
      "Iteration 267, loss = 0.32401180\n",
      "Iteration 268, loss = 0.32458592\n",
      "Iteration 269, loss = 0.32279628\n",
      "Iteration 270, loss = 0.32171952\n",
      "Iteration 271, loss = 0.32209966\n",
      "Iteration 272, loss = 0.32300542\n",
      "Iteration 273, loss = 0.32118978\n",
      "Iteration 274, loss = 0.32029763\n",
      "Iteration 275, loss = 0.31847184\n",
      "Iteration 276, loss = 0.31873620\n",
      "Iteration 277, loss = 0.31854051\n",
      "Iteration 278, loss = 0.31742702\n",
      "Iteration 279, loss = 0.31627588\n",
      "Iteration 280, loss = 0.31633413\n",
      "Iteration 281, loss = 0.31607196\n",
      "Iteration 282, loss = 0.31432362\n",
      "Iteration 283, loss = 0.31520885\n",
      "Iteration 284, loss = 0.31286807\n",
      "Iteration 285, loss = 0.31200619\n",
      "Iteration 286, loss = 0.31302740\n",
      "Iteration 287, loss = 0.31303142\n",
      "Iteration 288, loss = 0.31168888\n",
      "Iteration 289, loss = 0.30979018\n",
      "Iteration 290, loss = 0.30950851\n",
      "Iteration 291, loss = 0.30969863\n",
      "Iteration 292, loss = 0.30915715\n",
      "Iteration 293, loss = 0.30779470\n",
      "Iteration 294, loss = 0.30735933\n",
      "Iteration 295, loss = 0.30845255\n",
      "Iteration 296, loss = 0.30592124\n",
      "Iteration 297, loss = 0.30652489\n",
      "Iteration 298, loss = 0.30538086\n",
      "Iteration 299, loss = 0.30531207\n",
      "Iteration 300, loss = 0.30433620\n",
      "Iteration 301, loss = 0.30400337\n",
      "Iteration 302, loss = 0.30287770\n",
      "Iteration 303, loss = 0.30215005\n",
      "Iteration 304, loss = 0.30215042\n",
      "Iteration 305, loss = 0.30130663\n",
      "Iteration 306, loss = 0.30071359\n",
      "Iteration 307, loss = 0.30089849\n",
      "Iteration 308, loss = 0.29868996\n",
      "Iteration 309, loss = 0.29929134\n",
      "Iteration 310, loss = 0.29890449\n",
      "Iteration 311, loss = 0.29801742\n",
      "Iteration 312, loss = 0.29854168\n",
      "Iteration 313, loss = 0.29939625\n",
      "Iteration 314, loss = 0.29726306\n",
      "Iteration 315, loss = 0.29657089\n",
      "Iteration 316, loss = 0.29654515\n",
      "Iteration 317, loss = 0.29500711\n",
      "Iteration 318, loss = 0.29586263\n",
      "Iteration 319, loss = 0.29480629\n",
      "Iteration 320, loss = 0.29330713\n",
      "Iteration 321, loss = 0.29265661\n",
      "Iteration 322, loss = 0.29462827\n",
      "Iteration 323, loss = 0.29221365\n",
      "Iteration 324, loss = 0.29116456\n",
      "Iteration 325, loss = 0.29110418\n",
      "Iteration 326, loss = 0.29134205\n",
      "Iteration 327, loss = 0.28980039\n",
      "Iteration 328, loss = 0.28884273\n",
      "Iteration 329, loss = 0.28857030\n",
      "Iteration 330, loss = 0.28877168\n",
      "Iteration 331, loss = 0.28805687\n",
      "Iteration 332, loss = 0.28765755\n",
      "Iteration 333, loss = 0.28721432\n",
      "Iteration 334, loss = 0.28615762\n",
      "Iteration 335, loss = 0.28737126\n",
      "Iteration 336, loss = 0.28593207\n",
      "Iteration 337, loss = 0.28468202\n",
      "Iteration 338, loss = 0.28384494\n",
      "Iteration 339, loss = 0.28412071\n",
      "Iteration 340, loss = 0.28410681\n",
      "Iteration 341, loss = 0.28302286\n",
      "Iteration 342, loss = 0.28220892\n",
      "Iteration 343, loss = 0.28202844\n",
      "Iteration 344, loss = 0.28142744\n",
      "Iteration 345, loss = 0.28187985\n",
      "Iteration 346, loss = 0.28111245\n",
      "Iteration 347, loss = 0.27937316\n",
      "Iteration 348, loss = 0.27975543\n",
      "Iteration 349, loss = 0.27937785\n",
      "Iteration 350, loss = 0.27898106\n",
      "Iteration 351, loss = 0.27921814\n",
      "Iteration 352, loss = 0.27922525\n",
      "Iteration 353, loss = 0.27862047\n",
      "Iteration 354, loss = 0.27657496\n",
      "Iteration 355, loss = 0.27638720\n",
      "Iteration 356, loss = 0.27622280\n",
      "Iteration 357, loss = 0.27727156\n",
      "Iteration 358, loss = 0.27582629\n",
      "Iteration 359, loss = 0.27525819\n",
      "Iteration 360, loss = 0.27444719\n",
      "Iteration 361, loss = 0.27448657\n",
      "Iteration 362, loss = 0.27503269\n",
      "Iteration 363, loss = 0.27427716\n",
      "Iteration 364, loss = 0.27306190\n",
      "Iteration 365, loss = 0.27217138\n",
      "Iteration 366, loss = 0.27326289\n",
      "Iteration 367, loss = 0.27181973\n",
      "Iteration 368, loss = 0.27171358\n",
      "Iteration 369, loss = 0.27162136\n",
      "Iteration 370, loss = 0.27074448\n",
      "Iteration 371, loss = 0.26915602\n",
      "Iteration 372, loss = 0.27013756\n",
      "Iteration 373, loss = 0.27019680\n",
      "Iteration 374, loss = 0.26862440\n",
      "Iteration 375, loss = 0.26869654\n",
      "Iteration 376, loss = 0.26778110\n",
      "Iteration 377, loss = 0.26718930\n",
      "Iteration 378, loss = 0.26647226\n",
      "Iteration 379, loss = 0.26779645\n",
      "Iteration 380, loss = 0.26629693\n",
      "Iteration 381, loss = 0.26561923\n",
      "Iteration 382, loss = 0.26544090\n",
      "Iteration 383, loss = 0.26667210\n",
      "Iteration 384, loss = 0.26527914\n",
      "Iteration 385, loss = 0.26550749\n",
      "Iteration 386, loss = 0.26468028\n",
      "Iteration 387, loss = 0.26428981\n",
      "Iteration 388, loss = 0.26367772\n",
      "Iteration 389, loss = 0.26253835\n",
      "Iteration 390, loss = 0.26143606\n",
      "Iteration 391, loss = 0.26384627\n",
      "Iteration 392, loss = 0.26280391\n",
      "Iteration 393, loss = 0.26129333\n",
      "Iteration 394, loss = 0.26098513\n",
      "Iteration 395, loss = 0.26147346\n",
      "Iteration 396, loss = 0.26027058\n",
      "Iteration 397, loss = 0.25976695\n",
      "Iteration 398, loss = 0.25892855\n",
      "Iteration 399, loss = 0.25987923\n",
      "Iteration 400, loss = 0.25859232\n",
      "Iteration 401, loss = 0.25867631\n",
      "Iteration 402, loss = 0.25789345\n",
      "Iteration 403, loss = 0.25848228\n",
      "Iteration 404, loss = 0.25728710\n",
      "Iteration 405, loss = 0.25772978\n",
      "Iteration 406, loss = 0.25618534\n",
      "Iteration 407, loss = 0.25659893\n",
      "Iteration 408, loss = 0.25663052\n",
      "Iteration 409, loss = 0.25479754\n",
      "Iteration 410, loss = 0.25386359\n",
      "Iteration 411, loss = 0.25432220\n",
      "Iteration 412, loss = 0.25532510\n",
      "Iteration 413, loss = 0.25436129\n",
      "Iteration 414, loss = 0.25426246\n",
      "Iteration 415, loss = 0.25384562\n",
      "Iteration 416, loss = 0.25417838\n",
      "Iteration 417, loss = 0.25214579\n",
      "Iteration 418, loss = 0.25387491\n",
      "Iteration 419, loss = 0.25147455\n",
      "Iteration 420, loss = 0.25227286\n",
      "Iteration 421, loss = 0.25144403\n",
      "Iteration 422, loss = 0.25105599\n",
      "Iteration 423, loss = 0.25048666\n",
      "Iteration 424, loss = 0.24996965\n",
      "Iteration 425, loss = 0.24987274\n",
      "Iteration 426, loss = 0.25011791\n",
      "Iteration 427, loss = 0.24873728\n",
      "Iteration 428, loss = 0.24886598\n",
      "Iteration 429, loss = 0.24909479\n",
      "Iteration 430, loss = 0.24787333\n",
      "Iteration 431, loss = 0.24882577\n",
      "Iteration 432, loss = 0.24752103\n",
      "Iteration 433, loss = 0.24801844\n",
      "Iteration 434, loss = 0.24636324\n",
      "Iteration 435, loss = 0.24654941\n",
      "Iteration 436, loss = 0.24604291\n",
      "Iteration 437, loss = 0.24500095\n",
      "Iteration 438, loss = 0.24618936\n",
      "Iteration 439, loss = 0.24590013\n",
      "Iteration 440, loss = 0.24394590\n",
      "Iteration 441, loss = 0.24451453\n",
      "Iteration 442, loss = 0.24432307\n",
      "Iteration 443, loss = 0.24339611\n",
      "Iteration 444, loss = 0.24408895\n",
      "Iteration 445, loss = 0.24272808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:568: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69257683\n",
      "Iteration 2, loss = 0.68552278\n",
      "Iteration 3, loss = 0.68320572\n",
      "Iteration 4, loss = 0.68145468\n",
      "Iteration 5, loss = 0.68113202\n",
      "Iteration 6, loss = 0.68013501\n",
      "Iteration 7, loss = 0.67933595\n",
      "Iteration 8, loss = 0.67867099\n",
      "Iteration 9, loss = 0.67759937\n",
      "Iteration 10, loss = 0.67749000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:568: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69289293\n",
      "Iteration 2, loss = 0.68492086\n",
      "Iteration 3, loss = 0.68340007\n",
      "Iteration 4, loss = 0.68289615\n",
      "Iteration 5, loss = 0.68175921\n",
      "Iteration 6, loss = 0.68109524\n",
      "Iteration 7, loss = 0.67992396\n",
      "Iteration 8, loss = 0.67933431\n",
      "Iteration 9, loss = 0.67874243\n",
      "Iteration 10, loss = 0.67794239\n",
      "Iteration 11, loss = 0.67706437\n",
      "Iteration 12, loss = 0.67602138\n",
      "Iteration 13, loss = 0.67532949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:568: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69259769\n",
      "Iteration 2, loss = 0.68534657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:568: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69260056\n",
      "Iteration 2, loss = 0.68555068\n",
      "Iteration 3, loss = 0.68324064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:568: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69312616\n",
      "Iteration 2, loss = 0.68519848\n",
      "Iteration 3, loss = 0.68373013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:568: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3319, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-87b481c3eb46>\", line 31, in <module>\n",
      "    model.fit(X_trainf,y_train)\n",
      "  File \"C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\site-packages\\sklearn\\model_selection\\_search.py\", line 688, in fit\n",
      "    self._run_search(evaluate_candidates)\n",
      "  File \"C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\site-packages\\sklearn\\model_selection\\_search.py\", line 1149, in _run_search\n",
      "    evaluate_candidates(ParameterGrid(self.param_grid))\n",
      "  File \"C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\site-packages\\sklearn\\model_selection\\_search.py\", line 667, in evaluate_candidates\n",
      "    cv.split(X, y, groups)))\n",
      "  File \"C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\site-packages\\joblib\\parallel.py\", line 1007, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\site-packages\\joblib\\parallel.py\", line 835, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\site-packages\\joblib\\parallel.py\", line 754, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 209, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 590, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\site-packages\\joblib\\parallel.py\", line 256, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\site-packages\\joblib\\parallel.py\", line 256, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 556, in _fit_and_score\n",
      "    test_scores = _score(estimator, X_test, y_test, scorer, is_multimetric)\n",
      "  File \"C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 599, in _score\n",
      "    return _multimetric_score(estimator, X_test, y_test, scorer)\n",
      "  File \"C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 629, in _multimetric_score\n",
      "    score = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\site-packages\\sklearn\\metrics\\scorer.py\", line 90, in __call__\n",
      "    y_pred = estimator.predict(X)\n",
      "  File \"C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\", line 958, in predict\n",
      "    y_pred = self._predict(X)\n",
      "  File \"C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\", line 680, in _predict\n",
      "    self._forward_pass(activations)\n",
      "  File \"C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\", line 106, in _forward_pass\n",
      "    activations[i + 1] = hidden_activation(activations[i + 1])\n",
      "  File \"C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\site-packages\\sklearn\\neural_network\\_base.py\", line 43, in logistic\n",
      "    return logistic_sigmoid(X, out=X)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2034, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"C:\\Users\\Nurlaili\\Anaconda3\\envs\\iroschoolpython\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "##Import library\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "##Deciding the model configuration\n",
    "#make scorer for grid search\n",
    "def tp(y_true, y_pred):\n",
    "    error= confusion_matrix(y_true,y_pred)[0,0]/(confusion_matrix(y_true, y_pred)[0,0] + confusion_matrix(y_true, y_pred)[0,1])\n",
    "    return error\n",
    "\n",
    "specificity = make_scorer(tp, greater_is_better=True)\n",
    "scoring={'Accuracy': make_scorer(accuracy_score),'Precision': make_scorer(precision_score),'Recall': make_scorer(recall_score),'Specificity': specificity}\n",
    "\n",
    "#Grid search parameters\n",
    "parameters = {'activation':('logistic','tanh',\n",
    "'relu'),'alpha':(0.00001,0.0001,0.001,0.01,0.1,1)}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#Model configuration\n",
    "model = GridSearchCV(MLPClassifier(hidden_layer_sizes=(607,121),max_iter=500,\n",
    "                                   solver='adam',random_state=42,verbose=True),\n",
    "                                param_grid=parameters,scoring=scoring,refit=False,return_train_score=False)\n",
    "\n",
    "#Fit the model\n",
    "model.fit(X_trainf,y_train)\n",
    "\n",
    "#Results\n",
    "results = model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "             estimator=MLPClassifier(activation='relu', alpha=0.0001,\n",
       "                                     batch_size='auto', beta_1=0.9,\n",
       "                                     beta_2=0.999, early_stopping=False,\n",
       "                                     epsilon=1e-08,\n",
       "                                     hidden_layer_sizes=(607, 121),\n",
       "                                     learning_rate='constant',\n",
       "                                     learning_rate_init=0.001, max_iter=500,\n",
       "                                     momentum=0.9, n_iter_no_change=10,\n",
       "                                     nesterovs_momentum=True, power_t=0.5,\n",
       "                                     ra...\n",
       "                                     warm_start=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'activation': ('logistic', 'tanh', 'relu'),\n",
       "                         'alpha': (1e-05, 0.0001, 0.001, 0.01, 0.1, 1)},\n",
       "             pre_dispatch='2*n_jobs', refit=False, return_train_score=False,\n",
       "             scoring={'Accuracy': make_scorer(accuracy_score),\n",
       "                      'Precision': make_scorer(precision_score),\n",
       "                      'Recall': make_scorer(recall_score),\n",
       "                      'Specificity': make_scorer(tp)},\n",
       "             verbose=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.68701128\n",
      "Iteration 2, loss = 0.66445532\n",
      "Iteration 3, loss = 0.64690401\n",
      "Iteration 4, loss = 0.62965586\n",
      "Iteration 5, loss = 0.61237938\n",
      "Iteration 6, loss = 0.59662466\n",
      "Iteration 7, loss = 0.57839911\n",
      "Iteration 8, loss = 0.56273056\n",
      "Iteration 9, loss = 0.54880677\n",
      "Iteration 10, loss = 0.53557674\n",
      "Iteration 11, loss = 0.52350572\n",
      "Iteration 12, loss = 0.50940764\n",
      "Iteration 13, loss = 0.49755262\n",
      "Iteration 14, loss = 0.48479831\n",
      "Iteration 15, loss = 0.47396151\n",
      "Iteration 16, loss = 0.46427547\n",
      "Iteration 17, loss = 0.45450955\n",
      "Iteration 18, loss = 0.44230875\n",
      "Iteration 19, loss = 0.43313205\n",
      "Iteration 20, loss = 0.42417624\n",
      "Iteration 21, loss = 0.41459916\n",
      "Iteration 22, loss = 0.40567104\n",
      "Iteration 23, loss = 0.39855962\n",
      "Iteration 24, loss = 0.38915468\n",
      "Iteration 25, loss = 0.38276164\n",
      "Iteration 26, loss = 0.37263301\n",
      "Iteration 27, loss = 0.36758591\n",
      "Iteration 28, loss = 0.35839380\n",
      "Iteration 29, loss = 0.35330703\n",
      "Iteration 30, loss = 0.34609610\n",
      "Iteration 31, loss = 0.33903606\n",
      "Iteration 32, loss = 0.33459866\n",
      "Iteration 33, loss = 0.32709992\n",
      "Iteration 34, loss = 0.32376159\n",
      "Iteration 35, loss = 0.31744429\n",
      "Iteration 36, loss = 0.31298673\n",
      "Iteration 37, loss = 0.30562199\n",
      "Iteration 38, loss = 0.30233267\n",
      "Iteration 39, loss = 0.29826868\n",
      "Iteration 40, loss = 0.29259405\n",
      "Iteration 41, loss = 0.28693041\n",
      "Iteration 42, loss = 0.28270274\n",
      "Iteration 43, loss = 0.28034523\n",
      "Iteration 44, loss = 0.27530144\n",
      "Iteration 45, loss = 0.27194142\n",
      "Iteration 46, loss = 0.26827231\n",
      "Iteration 47, loss = 0.26382626\n",
      "Iteration 48, loss = 0.25962887\n",
      "Iteration 49, loss = 0.25684142\n",
      "Iteration 50, loss = 0.25388899\n",
      "Iteration 51, loss = 0.25065356\n",
      "Iteration 52, loss = 0.24767639\n",
      "Iteration 53, loss = 0.24468110\n",
      "Iteration 54, loss = 0.24095457\n",
      "Iteration 55, loss = 0.23786022\n",
      "Iteration 56, loss = 0.23587568\n",
      "Iteration 57, loss = 0.23394714\n",
      "Iteration 58, loss = 0.23125680\n",
      "Iteration 59, loss = 0.22676432\n",
      "Iteration 60, loss = 0.22532041\n",
      "Iteration 61, loss = 0.22321020\n",
      "Iteration 62, loss = 0.21995422\n",
      "Iteration 63, loss = 0.21937839\n",
      "Iteration 64, loss = 0.21609784\n",
      "Iteration 65, loss = 0.21348307\n",
      "Iteration 66, loss = 0.21258665\n",
      "Iteration 67, loss = 0.21002678\n",
      "Iteration 68, loss = 0.20790961\n",
      "Iteration 69, loss = 0.20581349\n",
      "Iteration 70, loss = 0.20452128\n",
      "Iteration 71, loss = 0.20126897\n",
      "Iteration 72, loss = 0.20079256\n",
      "Iteration 73, loss = 0.19873615\n",
      "Iteration 74, loss = 0.19730245\n",
      "Iteration 75, loss = 0.19558431\n",
      "Iteration 76, loss = 0.19204002\n",
      "Iteration 77, loss = 0.19274177\n",
      "Iteration 78, loss = 0.18906334\n",
      "Iteration 79, loss = 0.18807772\n",
      "Iteration 80, loss = 0.18824075\n",
      "Iteration 81, loss = 0.18578215\n",
      "Iteration 82, loss = 0.18440164\n",
      "Iteration 83, loss = 0.18312342\n",
      "Iteration 84, loss = 0.18183663\n",
      "Iteration 85, loss = 0.18093480\n",
      "Iteration 86, loss = 0.17941381\n",
      "Iteration 87, loss = 0.17768074\n",
      "Iteration 88, loss = 0.17797646\n",
      "Iteration 89, loss = 0.17622723\n",
      "Iteration 90, loss = 0.17478229\n",
      "Iteration 91, loss = 0.17274666\n",
      "Iteration 92, loss = 0.17111587\n",
      "Iteration 93, loss = 0.17015598\n",
      "Iteration 94, loss = 0.17060159\n",
      "Iteration 95, loss = 0.16849671\n",
      "Iteration 96, loss = 0.16728111\n",
      "Iteration 97, loss = 0.16479555\n",
      "Iteration 98, loss = 0.16628619\n",
      "Iteration 99, loss = 0.16469723\n",
      "Iteration 100, loss = 0.16311052\n",
      "Iteration 101, loss = 0.16291450\n",
      "Iteration 102, loss = 0.16147690\n",
      "Iteration 103, loss = 0.16014848\n",
      "Iteration 104, loss = 0.16114901\n",
      "Iteration 105, loss = 0.15875978\n",
      "Iteration 106, loss = 0.15903818\n",
      "Iteration 107, loss = 0.15852156\n",
      "Iteration 108, loss = 0.15734891\n",
      "Iteration 109, loss = 0.15629372\n",
      "Iteration 110, loss = 0.15564352\n",
      "Iteration 111, loss = 0.15441656\n",
      "Iteration 112, loss = 0.15559918\n",
      "Iteration 113, loss = 0.15412414\n",
      "Iteration 114, loss = 0.15222262\n",
      "Iteration 115, loss = 0.15050197\n",
      "Iteration 116, loss = 0.14955872\n",
      "Iteration 117, loss = 0.15034164\n",
      "Iteration 118, loss = 0.15078777\n",
      "Iteration 119, loss = 0.14927170\n",
      "Iteration 120, loss = 0.14870093\n",
      "Iteration 121, loss = 0.14846258\n",
      "Iteration 122, loss = 0.14837604\n",
      "Iteration 123, loss = 0.14612985\n",
      "Iteration 124, loss = 0.14623291\n",
      "Iteration 125, loss = 0.14529641\n",
      "Iteration 126, loss = 0.14430628\n",
      "Iteration 127, loss = 0.14444447\n",
      "Iteration 128, loss = 0.14344494\n",
      "Iteration 129, loss = 0.14551014\n",
      "Iteration 130, loss = 0.14207911\n",
      "Iteration 131, loss = 0.14268377\n",
      "Iteration 132, loss = 0.14198929\n",
      "Iteration 133, loss = 0.14126195\n",
      "Iteration 134, loss = 0.14251405\n",
      "Iteration 135, loss = 0.14160031\n",
      "Iteration 136, loss = 0.13979578\n",
      "Iteration 137, loss = 0.13926544\n",
      "Iteration 138, loss = 0.14087604\n",
      "Iteration 139, loss = 0.13867944\n",
      "Iteration 140, loss = 0.13926513\n",
      "Iteration 141, loss = 0.13654648\n",
      "Iteration 142, loss = 0.13728296\n",
      "Iteration 143, loss = 0.13670103\n",
      "Iteration 144, loss = 0.13723121\n",
      "Iteration 145, loss = 0.13662071\n",
      "Iteration 146, loss = 0.13625509\n",
      "Iteration 147, loss = 0.13630666\n",
      "Iteration 148, loss = 0.13503571\n",
      "Iteration 149, loss = 0.13590237\n",
      "Iteration 150, loss = 0.13444969\n",
      "Iteration 151, loss = 0.13342576\n",
      "Iteration 152, loss = 0.13481856\n",
      "Iteration 153, loss = 0.13366248\n",
      "Iteration 154, loss = 0.13334461\n",
      "Iteration 155, loss = 0.13254181\n",
      "Iteration 156, loss = 0.13231752\n",
      "Iteration 157, loss = 0.13361168\n",
      "Iteration 158, loss = 0.13263613\n",
      "Iteration 159, loss = 0.13033490\n",
      "Iteration 160, loss = 0.13005573\n",
      "Iteration 161, loss = 0.13135285\n",
      "Iteration 162, loss = 0.13068248\n",
      "Iteration 163, loss = 0.12986096\n",
      "Iteration 164, loss = 0.12888599\n",
      "Iteration 165, loss = 0.12900629\n",
      "Iteration 166, loss = 0.13038951\n",
      "Iteration 167, loss = 0.12959445\n",
      "Iteration 168, loss = 0.12818435\n",
      "Iteration 169, loss = 0.12887164\n",
      "Iteration 170, loss = 0.12789296\n",
      "Iteration 171, loss = 0.12677433\n",
      "Iteration 172, loss = 0.12779556\n",
      "Iteration 173, loss = 0.12899168\n",
      "Iteration 174, loss = 0.12619144\n",
      "Iteration 175, loss = 0.12742313\n",
      "Iteration 176, loss = 0.12564399\n",
      "Iteration 177, loss = 0.12582529\n",
      "Iteration 178, loss = 0.12646874\n",
      "Iteration 179, loss = 0.12555678\n",
      "Iteration 180, loss = 0.12372450\n",
      "Iteration 181, loss = 0.12499809\n",
      "Iteration 182, loss = 0.12551740\n",
      "Iteration 183, loss = 0.12495950\n",
      "Iteration 184, loss = 0.12383532\n",
      "Iteration 185, loss = 0.12408527\n",
      "Iteration 186, loss = 0.12395627\n",
      "Iteration 187, loss = 0.12421720\n",
      "Iteration 188, loss = 0.12344803\n",
      "Iteration 189, loss = 0.12385078\n",
      "Iteration 190, loss = 0.12283926\n",
      "Iteration 191, loss = 0.12431288\n",
      "Iteration 192, loss = 0.12241852\n",
      "Iteration 193, loss = 0.12245256\n",
      "Iteration 194, loss = 0.12257050\n",
      "Iteration 195, loss = 0.12196701\n",
      "Iteration 196, loss = 0.12157053\n",
      "Iteration 197, loss = 0.12209991\n",
      "Iteration 198, loss = 0.12239551\n",
      "Iteration 199, loss = 0.12347404\n",
      "Iteration 200, loss = 0.12160526\n",
      "Iteration 201, loss = 0.11951083\n",
      "Iteration 202, loss = 0.12151459\n",
      "Iteration 203, loss = 0.12020996\n",
      "Iteration 204, loss = 0.11955881\n",
      "Iteration 205, loss = 0.12169067\n",
      "Iteration 206, loss = 0.12001252\n",
      "Iteration 207, loss = 0.12089756\n",
      "Iteration 208, loss = 0.11926719\n",
      "Iteration 209, loss = 0.11911221\n",
      "Iteration 210, loss = 0.12189784\n",
      "Iteration 211, loss = 0.12067929\n",
      "Iteration 212, loss = 0.12091293\n",
      "Iteration 213, loss = 0.11993479\n",
      "Iteration 214, loss = 0.12096386\n",
      "Iteration 215, loss = 0.11725981\n",
      "Iteration 216, loss = 0.11874122\n",
      "Iteration 217, loss = 0.11811536\n",
      "Iteration 218, loss = 0.11704190\n",
      "Iteration 219, loss = 0.11580108\n",
      "Iteration 220, loss = 0.11837583\n",
      "Iteration 221, loss = 0.11756622\n",
      "Iteration 222, loss = 0.11899418\n",
      "Iteration 223, loss = 0.11647581\n",
      "Iteration 224, loss = 0.11685778\n",
      "Iteration 225, loss = 0.11965603\n",
      "Iteration 226, loss = 0.11777627\n",
      "Iteration 227, loss = 0.11727520\n",
      "Iteration 228, loss = 0.11663794\n",
      "Iteration 229, loss = 0.11596027\n",
      "Iteration 230, loss = 0.11594386\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[[  14  269]\n",
      " [ 521 5016]]\n"
     ]
    }
   ],
   "source": [
    "##Testing\n",
    "#Model configuration\n",
    "model=MLPClassifier(hidden_layer_sizes=(607,121),activation='tanh',max_iter=500,solver='adam',verbose=True,alpha=0.0001)\n",
    "\n",
    "#Fit model\n",
    "model.fit(X_trainf,y_train)\n",
    "loss_values = model.loss_curve_\n",
    "\n",
    "#Test the model\n",
    "y_pred = model.predict(X_testf)\n",
    "\n",
    "#Print final result\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.03      0.05      0.03       283\n",
      "           1       0.95      0.91      0.93      5537\n",
      "\n",
      "    accuracy                           0.86      5820\n",
      "   macro avg       0.49      0.48      0.48      5820\n",
      "weighted avg       0.90      0.86      0.88      5820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
